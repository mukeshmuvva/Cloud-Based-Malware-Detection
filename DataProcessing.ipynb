{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Colab_Training_F21.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVhbcqQMbApz"
      },
      "source": [
        "Install the above lief version for smoother execution of your project\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJsBexOObLUD",
        "outputId": "a319671d-5005-476a-be2b-b19c9c26bf7c"
      },
      "source": [
        "!pip install lief==0.10.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lief==0.10.0\n",
            "  Downloading lief-0.10.0-cp37-cp37m-manylinux1_x86_64.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 5.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: lief\n",
            "Successfully installed lief-0.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esNWMzKrVuWc"
      },
      "source": [
        "**Step 1:** Mount your Google Drive by clicking on \"Mount Drive\" in the Files section (panel to the left of this text.)\n",
        "\n",
        "**Step 2:** Go to Runtime -> Change runtime type and select TPU.\n",
        "\n",
        "**Step 3:** Create a folder in your Google Drive, and rename it to \"vMalConv\"\n",
        "\n",
        "**Step 4:** Download the pre-processed training and test datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUq_FZwmZegw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24879a7a-c593-42cd-eeb7-a8329d60562c"
      },
      "source": [
        "!wget https://dsci6015aisecf21.s3.us-east-2.amazonaws.com/X_train.dat\n",
        "!wget https://dsci6015aisecf21.s3.us-east-2.amazonaws.com/X_test.dat\n",
        "!wget https://dsci6015aisecf21.s3.us-east-2.amazonaws.com/y_train.dat\n",
        "!wget https://dsci6015aisecf21.s3.us-east-2.amazonaws.com/y_test.dat\n",
        "!wget https://dsci6015aisecf21.s3.us-east-2.amazonaws.com/metadata.csv"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-13 20:18:50--  https://dsci6015aisecf21.s3.us-east-2.amazonaws.com/X_train.dat\n",
            "Resolving dsci6015aisecf21.s3.us-east-2.amazonaws.com (dsci6015aisecf21.s3.us-east-2.amazonaws.com)... 52.219.143.34\n",
            "Connecting to dsci6015aisecf21.s3.us-east-2.amazonaws.com (dsci6015aisecf21.s3.us-east-2.amazonaws.com)|52.219.143.34|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8571600000 (8.0G) [application/x-www-form-urlencoded]\n",
            "Saving to: ‘X_train.dat’\n",
            "\n",
            "X_train.dat         100%[===================>]   7.98G  95.3MB/s    in 86s     \n",
            "\n",
            "2021-10-13 20:20:16 (95.4 MB/s) - ‘X_train.dat’ saved [8571600000/8571600000]\n",
            "\n",
            "--2021-10-13 20:20:16--  https://dsci6015aisecf21.s3.us-east-2.amazonaws.com/X_test.dat\n",
            "Resolving dsci6015aisecf21.s3.us-east-2.amazonaws.com (dsci6015aisecf21.s3.us-east-2.amazonaws.com)... 52.219.102.194\n",
            "Connecting to dsci6015aisecf21.s3.us-east-2.amazonaws.com (dsci6015aisecf21.s3.us-east-2.amazonaws.com)|52.219.102.194|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1904800000 (1.8G) [application/x-www-form-urlencoded]\n",
            "Saving to: ‘X_test.dat’\n",
            "\n",
            "X_test.dat          100%[===================>]   1.77G  95.5MB/s    in 19s     \n",
            "\n",
            "2021-10-13 20:20:35 (95.5 MB/s) - ‘X_test.dat’ saved [1904800000/1904800000]\n",
            "\n",
            "--2021-10-13 20:20:35--  https://dsci6015aisecf21.s3.us-east-2.amazonaws.com/y_train.dat\n",
            "Resolving dsci6015aisecf21.s3.us-east-2.amazonaws.com (dsci6015aisecf21.s3.us-east-2.amazonaws.com)... 52.219.143.2\n",
            "Connecting to dsci6015aisecf21.s3.us-east-2.amazonaws.com (dsci6015aisecf21.s3.us-east-2.amazonaws.com)|52.219.143.2|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3600000 (3.4M) [binary/octet-stream]\n",
            "Saving to: ‘y_train.dat’\n",
            "\n",
            "y_train.dat         100%[===================>]   3.43M  15.5MB/s    in 0.2s    \n",
            "\n",
            "2021-10-13 20:20:36 (15.5 MB/s) - ‘y_train.dat’ saved [3600000/3600000]\n",
            "\n",
            "--2021-10-13 20:20:36--  https://dsci6015aisecf21.s3.us-east-2.amazonaws.com/y_test.dat\n",
            "Resolving dsci6015aisecf21.s3.us-east-2.amazonaws.com (dsci6015aisecf21.s3.us-east-2.amazonaws.com)... 52.219.143.2\n",
            "Connecting to dsci6015aisecf21.s3.us-east-2.amazonaws.com (dsci6015aisecf21.s3.us-east-2.amazonaws.com)|52.219.143.2|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 800000 (781K) [binary/octet-stream]\n",
            "Saving to: ‘y_test.dat’\n",
            "\n",
            "y_test.dat          100%[===================>] 781.25K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2021-10-13 20:20:36 (5.68 MB/s) - ‘y_test.dat’ saved [800000/800000]\n",
            "\n",
            "--2021-10-13 20:20:36--  https://dsci6015aisecf21.s3.us-east-2.amazonaws.com/metadata.csv\n",
            "Resolving dsci6015aisecf21.s3.us-east-2.amazonaws.com (dsci6015aisecf21.s3.us-east-2.amazonaws.com)... 52.219.143.2\n",
            "Connecting to dsci6015aisecf21.s3.us-east-2.amazonaws.com (dsci6015aisecf21.s3.us-east-2.amazonaws.com)|52.219.143.2|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 96888920 (92M) [text/csv]\n",
            "Saving to: ‘metadata.csv’\n",
            "\n",
            "metadata.csv        100%[===================>]  92.40M  87.5MB/s    in 1.1s    \n",
            "\n",
            "2021-10-13 20:20:37 (87.5 MB/s) - ‘metadata.csv’ saved [96888920/96888920]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQ_JdZKfG7Q-",
        "outputId": "40a20795-24b0-4a24-fa1b-344bd9719067"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9V958PbDW3H0"
      },
      "source": [
        "**Step 5:** Copy the downloaded files to vMalConv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llip77F3amma"
      },
      "source": [
        "!cp /content/X_train.dat /content/drive/MyDrive/vMalConv/X_train.dat\n",
        "!cp /content/X_test.dat /content/drive/MyDrive/vMalConv/X_test.dat\n",
        "!cp /content/y_train.dat /content/drive/MyDrive/vMalConv/y_train.dat\n",
        "!cp /content/y_test.dat /content/drive/MyDrive/vMalConv/y_test.dat\n",
        "!cp /content/metadata.csv /content/drive/MyDrive/vMalConv/metadata.csv"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbRilyqTXnrE"
      },
      "source": [
        "**Step 6:** Download and install Ember:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76bc7PEmlwKB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6d5c47e-924d-4912-d08b-c0e854deb4e0"
      },
      "source": [
        "!wget https://github.com/endgameinc/ember/archive/master.zip\n",
        "!unzip master.zip\n",
        "!rm master.zip\n",
        "!cp -r ember-master/* .\n",
        "!rm -r ember-master\n",
        "!pip install -r requirements.txt\n",
        "!python setup.py install"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-14 10:52:06--  https://github.com/endgameinc/ember/archive/master.zip\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://github.com/elastic/ember/archive/master.zip [following]\n",
            "--2021-10-14 10:52:06--  https://github.com/elastic/ember/archive/master.zip\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://codeload.github.com/elastic/ember/zip/master [following]\n",
            "--2021-10-14 10:52:06--  https://codeload.github.com/elastic/ember/zip/master\n",
            "Resolving codeload.github.com (codeload.github.com)... 140.82.113.10\n",
            "Connecting to codeload.github.com (codeload.github.com)|140.82.113.10|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: ‘master.zip’\n",
            "\n",
            "master.zip              [   <=>              ]  11.22M  12.1MB/s    in 0.9s    \n",
            "\n",
            "2021-10-14 10:52:07 (12.1 MB/s) - ‘master.zip’ saved [11769696]\n",
            "\n",
            "Archive:  master.zip\n",
            "4dee42918694d72d319e731940755146a71f5c6c\n",
            "   creating: ember-master/\n",
            "  inflating: ember-master/LICENSE.txt  \n",
            "  inflating: ember-master/README.md  \n",
            "   creating: ember-master/ember/\n",
            "  inflating: ember-master/ember/__init__.py  \n",
            "  inflating: ember-master/ember/features.py  \n",
            "   creating: ember-master/licenses/\n",
            "  inflating: ember-master/licenses/AGPL-LICENSE-3.0.txt  \n",
            "  inflating: ember-master/licenses/MIT-LICENSE.txt  \n",
            "   creating: ember-master/malconv/\n",
            "  inflating: ember-master/malconv/README.md  \n",
            "  inflating: ember-master/malconv/malconv.h5  \n",
            "  inflating: ember-master/malconv/malconv.py  \n",
            "  inflating: ember-master/malconv/multi_gpu.py  \n",
            "  inflating: ember-master/requirements.txt  \n",
            "  inflating: ember-master/requirements_conda.txt  \n",
            "  inflating: ember-master/requirements_notebook.txt  \n",
            "   creating: ember-master/resources/\n",
            "  inflating: ember-master/resources/ember-notebook.ipynb  \n",
            "  inflating: ember-master/resources/ember2018-notebook.ipynb  \n",
            "  inflating: ember-master/resources/logo.png  \n",
            "   creating: ember-master/scripts/\n",
            "  inflating: ember-master/scripts/classify_binaries.py  \n",
            "  inflating: ember-master/scripts/init_ember.py  \n",
            "  inflating: ember-master/setup.py   \n",
            "Requirement already satisfied: lief>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (0.10.0)\n",
            "Requirement already satisfied: tqdm>=4.31.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (4.62.3)\n",
            "Requirement already satisfied: numpy>=1.16.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (1.19.5)\n",
            "Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (1.1.5)\n",
            "Requirement already satisfied: lightgbm>=2.2.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (2.2.3)\n",
            "Requirement already satisfied: scikit-learn>=0.20.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (0.22.2.post1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.2->-r requirements.txt (line 4)) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.2->-r requirements.txt (line 4)) (2.8.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from lightgbm>=2.2.3->-r requirements.txt (line 5)) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.3->-r requirements.txt (line 6)) (1.0.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24.2->-r requirements.txt (line 4)) (1.15.0)\n",
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "writing ember.egg-info/PKG-INFO\n",
            "writing dependency_links to ember.egg-info/dependency_links.txt\n",
            "writing requirements to ember.egg-info/requires.txt\n",
            "writing top-level names to ember.egg-info/top_level.txt\n",
            "reading manifest file 'ember.egg-info/SOURCES.txt'\n",
            "adding license file 'LICENSE.txt'\n",
            "writing manifest file 'ember.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "copying ember/features.py -> build/lib/ember\n",
            "copying ember/__init__.py -> build/lib/ember\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/ember\n",
            "copying build/lib/ember/features.py -> build/bdist.linux-x86_64/egg/ember\n",
            "copying build/lib/ember/__init__.py -> build/bdist.linux-x86_64/egg/ember\n",
            "byte-compiling build/bdist.linux-x86_64/egg/ember/features.py to features.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/ember/__init__.py to __init__.cpython-37.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying ember.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying ember.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying ember.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying ember.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying ember.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "creating 'dist/ember-0.1.0-py3.7.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing ember-0.1.0-py3.7.egg\n",
            "Removing /usr/local/lib/python3.7/dist-packages/ember-0.1.0-py3.7.egg\n",
            "Copying ember-0.1.0-py3.7.egg to /usr/local/lib/python3.7/dist-packages\n",
            "ember 0.1.0 is already the active version in easy-install.pth\n",
            "\n",
            "Installed /usr/local/lib/python3.7/dist-packages/ember-0.1.0-py3.7.egg\n",
            "Processing dependencies for ember==0.1.0\n",
            "Searching for scikit-learn==0.22.2.post1\n",
            "Best match: scikit-learn 0.22.2.post1\n",
            "Adding scikit-learn 0.22.2.post1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for lightgbm==2.2.3\n",
            "Best match: lightgbm 2.2.3\n",
            "Adding lightgbm 2.2.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for pandas==1.1.5\n",
            "Best match: pandas 1.1.5\n",
            "Adding pandas 1.1.5 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for numpy==1.19.5\n",
            "Best match: numpy 1.19.5\n",
            "Adding numpy 1.19.5 to easy-install.pth file\n",
            "Installing f2py script to /usr/local/bin\n",
            "Installing f2py3 script to /usr/local/bin\n",
            "Installing f2py3.7 script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for tqdm==4.62.3\n",
            "Best match: tqdm 4.62.3\n",
            "Adding tqdm 4.62.3 to easy-install.pth file\n",
            "Installing tqdm script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for lief==0.10.0\n",
            "Best match: lief 0.10.0\n",
            "Adding lief 0.10.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for scipy==1.4.1\n",
            "Best match: scipy 1.4.1\n",
            "Adding scipy 1.4.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for joblib==1.0.1\n",
            "Best match: joblib 1.0.1\n",
            "Adding joblib 1.0.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for python-dateutil==2.8.2\n",
            "Best match: python-dateutil 2.8.2\n",
            "Adding python-dateutil 2.8.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for pytz==2018.9\n",
            "Best match: pytz 2018.9\n",
            "Adding pytz 2018.9 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for six==1.15.0\n",
            "Best match: six 1.15.0\n",
            "Adding six 1.15.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Finished processing dependencies for ember==0.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXym5qd8Yv8f"
      },
      "source": [
        "**Step 7:** Read vectorized features from the data files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfcHyoTsmCFH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9859dc64-1bbc-4870-f3c3-ba3e87f972fd"
      },
      "source": [
        "import ember\n",
        "X_train, y_train, X_test, y_test = ember.read_vectorized_features(\"drive/MyDrive/vMalConv/\")\n",
        "metadata_dataframe = ember.read_metadata(\"drive/MyDrive/vMalConv/\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: EMBER feature version 2 were computed using lief version 0.9.0-\n",
            "WARNING:   lief version 0.10.0-845f675 found instead. There may be slight inconsistencies\n",
            "WARNING:   in the feature calculations.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/lib/arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  mask |= (ar1 == a)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTRCz7m7Z7EH"
      },
      "source": [
        "**Step 7:** Get rid of rows with no labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zj63lcvin44q"
      },
      "source": [
        "labelrows = (y_train != -1)\n",
        "X_train = X_train[labelrows]\n",
        "y_train = y_train[labelrows]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVG59AGooyC5"
      },
      "source": [
        "import h5py\n",
        "h5f = h5py.File('X_train.h5', 'w')\n",
        "h5f.create_dataset('X_train', data=X_train)\n",
        "h5f.close()\n",
        "h5f = h5py.File('y_train.h5', 'w')\n",
        "h5f.create_dataset('y_train', data=y_train)\n",
        "h5f.close()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tmUIJNvpZch",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1475d91-1467-4eaf-c6c2-98faadcc96fa"
      },
      "source": [
        "!cp /content/X_train.h5 /content/drive/MyDrive/vMalConv/X_train.h5\n",
        "!cp /content/y_train.h5 /content/drive/MyDrive/vMalConv/y_train.h5"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot create regular file '/content/drive/MyDrive/vMalConv/X_train.h5': Input/output error\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1bRlBWlaQdd"
      },
      "source": [
        " The following code to create the architecture of MalConv in Keras:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1UTVZi0qkGe"
      },
      "source": [
        "def make_model():\n",
        "  import tensorflow as tf\n",
        "  from tensorflow import keras\n",
        "  from tensorflow.keras import layers\n",
        "  feature_size=2381\n",
        "  tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "  keras.backend.clear_session()\n",
        "  \n",
        "  # Model architecture\n",
        "  from tensorflow.keras import layers\n",
        "  \n",
        " ### Your code -- Define the layers of MalConv ###\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(layers.InputLayer(input_shape=(1,feature_size)))\n",
        "  model.add(layers.Dropout(0.2))\n",
        "  model.add(layers.Dense(1500, activation='relu'))\n",
        "  model.add(layers.Dropout(0.5))\n",
        "  model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "  model.compile(tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "            loss='binary_crossentropy',\n",
        "            metrics=['accuracy',tf.keras.metrics.AUC(),tf.keras.metrics.Precision()])\n",
        "  print(model.summary())\n",
        "  return model"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1ZlKQwDv4uz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db08b114-731b-4437-eb6c-0269678e63d1"
      },
      "source": [
        "model = make_model()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dropout (Dropout)            (None, 1, 2381)           0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1, 1500)           3573000   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 1, 1500)           0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1, 1)              1501      \n",
            "=================================================================\n",
            "Total params: 3,574,501\n",
            "Trainable params: 3,574,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5YF17HqsFTS"
      },
      "source": [
        "\n",
        "save_dir = \"drive/MyDrive/vMalConv/\"\n",
        "model.save(save_dir+'model.h5')"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pihnLcFmbaet"
      },
      "source": [
        "**Step 8:** Partial fit the standardScaler to avoid overloading the memory:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4q5OfK9v9iN"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "mms = StandardScaler()\n",
        "for x in range(0,600000,100000):\n",
        "  mms.partial_fit(X_train[x:x+100000])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B33Oa1sTxdB0"
      },
      "source": [
        "X_train = mms.transform(X_train)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_vl5yrex0yY"
      },
      "source": [
        "## Reshape to create 3 channels ##\n",
        "import numpy as np\n",
        "X_train = np.reshape(X_train,(-1,1,2381))\n",
        "y_train = np.reshape(y_train,(-1,1,1))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zMgth6McCqV"
      },
      "source": [
        "> Train the model for 30 epochs, with a batch size of 128, and 20% validation split. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IncS7YgW6xJZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b55183a7-0d0f-4663-b1f4-3241e639765e"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "save_dir = \"drive/MyDrive/vMalConv/\"\n",
        "\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "\n",
        "\n",
        "model.compile(tf.keras.optimizers.Adam(learning_rate=0.01),\n",
        "          loss='binary_crossentropy',\n",
        "          metrics=['accuracy',tf.keras.metrics.AUC(),tf.keras.metrics.Precision()])\n",
        "\n",
        "history = model.fit(X_train, y_train,\n",
        "                batch_size=128,\n",
        "                epochs=30,\n",
        "                  validation_split=.2,\n",
        "                  callbacks=[callback]\n",
        "                  )\n",
        "# Save the weights #\n",
        "model.save_weights (save_dir+'weights.h5')\n",
        "\n",
        "# Save the model architecture #\n",
        "model_json = model.to_json()\n",
        "with open(save_dir+\"model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "print(\"model saved.\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train on 480000 samples, validate on 120000 samples\n",
            "Epoch 1/30\n",
            "479488/480000 [============================>.] - ETA: 0s - loss: 6.7863 - accuracy: 0.9407 - auc_1: 0.9468 - precision_1: 0.9397"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "480000/480000 [==============================] - 43s 90us/sample - loss: 6.7823 - accuracy: 0.9407 - auc_1: 0.9469 - precision_1: 0.9397 - val_loss: 2.6063 - val_accuracy: 0.9787 - val_auc_1: 0.9822 - val_precision_1: 0.9850\n",
            "Epoch 2/30\n",
            "480000/480000 [==============================] - 44s 92us/sample - loss: 8.9693 - accuracy: 0.9584 - auc_1: 0.9619 - precision_1: 0.9575 - val_loss: 3.8622 - val_accuracy: 0.9777 - val_auc_1: 0.9795 - val_precision_1: 0.9853\n",
            "Epoch 3/30\n",
            "480000/480000 [==============================] - 44s 91us/sample - loss: 9.4387 - accuracy: 0.9621 - auc_1: 0.9658 - precision_1: 0.9609 - val_loss: 5.8575 - val_accuracy: 0.9794 - val_auc_1: 0.9810 - val_precision_1: 0.9797\n",
            "Epoch 4/30\n",
            "480000/480000 [==============================] - 44s 92us/sample - loss: 8.3477 - accuracy: 0.9664 - auc_1: 0.9708 - precision_1: 0.9653 - val_loss: 3.2549 - val_accuracy: 0.9859 - val_auc_1: 0.9886 - val_precision_1: 0.9870\n",
            "Epoch 5/30\n",
            "480000/480000 [==============================] - 46s 97us/sample - loss: 8.6081 - accuracy: 0.9663 - auc_1: 0.9712 - precision_1: 0.9647 - val_loss: 5.7925 - val_accuracy: 0.9821 - val_auc_1: 0.9868 - val_precision_1: 0.9924\n",
            "Epoch 6/30\n",
            "480000/480000 [==============================] - 44s 92us/sample - loss: 7.8078 - accuracy: 0.9678 - auc_1: 0.9730 - precision_1: 0.9662 - val_loss: 8.6803 - val_accuracy: 0.9826 - val_auc_1: 0.9869 - val_precision_1: 0.9803\n",
            "Epoch 7/30\n",
            "480000/480000 [==============================] - 45s 94us/sample - loss: 9.8195 - accuracy: 0.9681 - auc_1: 0.9734 - precision_1: 0.9659 - val_loss: 7.5618 - val_accuracy: 0.9829 - val_auc_1: 0.9886 - val_precision_1: 0.9918\n",
            "Epoch 8/30\n",
            "480000/480000 [==============================] - 43s 90us/sample - loss: 10.7416 - accuracy: 0.9696 - auc_1: 0.9754 - precision_1: 0.9668 - val_loss: 8.6586 - val_accuracy: 0.9791 - val_auc_1: 0.9862 - val_precision_1: 0.9873\n",
            "Epoch 9/30\n",
            "480000/480000 [==============================] - 44s 92us/sample - loss: 11.5595 - accuracy: 0.9699 - auc_1: 0.9756 - precision_1: 0.9677 - val_loss: 6.9701 - val_accuracy: 0.9858 - val_auc_1: 0.9914 - val_precision_1: 0.9858\n",
            "Epoch 10/30\n",
            "480000/480000 [==============================] - 43s 89us/sample - loss: 8.7423 - accuracy: 0.9702 - auc_1: 0.9775 - precision_1: 0.9665 - val_loss: 15.4410 - val_accuracy: 0.9845 - val_auc_1: 0.9893 - val_precision_1: 0.9902\n",
            "Epoch 11/30\n",
            "480000/480000 [==============================] - 44s 93us/sample - loss: 11.7530 - accuracy: 0.9692 - auc_1: 0.9767 - precision_1: 0.9652 - val_loss: 8.2310 - val_accuracy: 0.9884 - val_auc_1: 0.9921 - val_precision_1: 0.9897\n",
            "model saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9S7f9aitfvHs"
      },
      "source": [
        "**Results time now !!!!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J01h0Ywuf1z3"
      },
      "source": [
        "mms = StandardScaler()\n",
        "\n",
        "X_test = mms.fit_transform(X_test)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z21-6v8sg0bk"
      },
      "source": [
        "X_test = np.reshape(X_test,(-1,1,2381))\n",
        "y_test = np.reshape(y_test,(-1,1,1))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dts5_R2fg80U"
      },
      "source": [
        "results =model.evaluate(X_test,y_test)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZWupTJqhLLk",
        "outputId": "e6982a7c-69cc-4a19-eef7-d9e2efe33624"
      },
      "source": [
        "print('Accuracy:',results[1])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.95049\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2I61TqRhTA7",
        "outputId": "94a159f5-3cce-4603-f6ad-42cbd2b5ed7d"
      },
      "source": [
        "print('Loss :', results[0])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss : 20.72967783213051\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2YxkEwNhe-T"
      },
      "source": [
        "**The time has came now for us to test with real world .exe on our model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1AJ_9zOiAC8"
      },
      "source": [
        "def testPE(pe): \n",
        "  import ember\n",
        "  import numpy as np\n",
        "  import tensorflow as tf\n",
        "  from sklearn.preprocessing import StandardScaler\n",
        "  mms = StandardScaler()\n",
        "  data = open(pe, \"rb\").read()\n",
        "  extractor = ember.PEFeatureExtractor()\n",
        "  data = np.array(extractor.feature_vector(data), dtype=np.float32)\n",
        "  data = mms.fit_transform([data])\n",
        "  data = np.reshape(data,(-1,1,2381))\n",
        "\n",
        "\n",
        "\n",
        "  model = tf.keras.models.load_model('/content/drive/MyDrive/data/model_n.h5')\n",
        "  pred = model.predict(data)\n",
        "\n",
        "  return pred"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTVttBGBiiRT",
        "outputId": "68c41be3-1017-40ed-c8ad-4b084678ea07"
      },
      "source": [
        "!wget https://web.archive.org/web/20070203100217/http://www.caltrox.com/downloads/down1/engword2.exe"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-14 11:26:43--  https://web.archive.org/web/20070203100217/http://www.caltrox.com/downloads/down1/engword2.exe\n",
            "Resolving web.archive.org (web.archive.org)... 207.241.237.3\n",
            "Connecting to web.archive.org (web.archive.org)|207.241.237.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/octet-stream]\n",
            "Saving to: ‘engword2.exe.1’\n",
            "\n",
            "engword2.exe.1          [  <=>               ]   2.28M  6.67MB/s    in 0.3s    \n",
            "\n",
            "2021-10-14 11:26:44 (6.67 MB/s) - ‘engword2.exe.1’ saved [2386971]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrfWCX9FimVz",
        "outputId": "c0765d32-c1bd-4800-ef78-eb17ae0d1ed5"
      },
      "source": [
        "testPE('engword2.exe')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: EMBER feature version 2 were computed using lief version 0.9.0-\n",
            "WARNING:   lief version 0.10.0-845f675 found instead. There may be slight inconsistencies\n",
            "WARNING:   in the feature calculations.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0.69268006]]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-XIZSjYlIL5"
      },
      "source": [
        "For a .exe file like \"engword2.exe\", I have gotten the output as 0.69, which is less than 0.5. so the answer for the above one is malign."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlD0i7Wflayi"
      },
      "source": [
        "Is it really malign? perhaps, because i took that file from an chrome advertisement. it says click here to get sometthing."
      ]
    }
  ]
}